{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4246862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83658bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 Implementation of the Oslo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b519da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries used below\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import sem\n",
    "from collections import OrderedDict, Counter\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e894958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logbin function from the python script provided on BB\n",
    "\n",
    "################################################################################\n",
    "# Max Falkenberg McGillivray\n",
    "# mff113@ic.ac.uk\n",
    "# 2019 Complexity & Networks course\n",
    "#\n",
    "# logbin230119.py v2.0\n",
    "# 23/01/2019\n",
    "# Email me if you find any bugs!\n",
    "#\n",
    "# For details on data binning see Appendix E from\n",
    "# K. Christensen and N.R. Moloney, Complexity and Criticality,\n",
    "# Imperial College Press (2005).\n",
    "################################################################################\n",
    "\n",
    "def logbin(data, scale = 1., zeros = False):\n",
    "    \"\"\"\n",
    "    logbin(data, scale = 1., zeros = False)\n",
    "\n",
    "    Log-bin frequency of unique integer values in data. Returns probabilities\n",
    "    for each bin.\n",
    "\n",
    "    Array, data, is a 1-d array containing full set of event sizes for a\n",
    "    given process in no particular order. For instance, in the Oslo Model\n",
    "    the array may contain the avalanche size recorded at each time step. For\n",
    "    a complex network, the array may contain the degree of each node in the\n",
    "    network. The logbin function finds the frequency of each unique value in\n",
    "    the data array. The function then bins these frequencies in logarithmically\n",
    "    increasing bin sizes controlled by the scale parameter.\n",
    "\n",
    "    Minimum binsize is always 1. Bin edges are lowered to nearest integer. Bins\n",
    "    are always unique, i.e. two different float bin edges corresponding to the\n",
    "    same integer interval will not be included twice. Note, rounding to integer\n",
    "    values results in noise at small event sizes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data: array_like, 1 dimensional, non-negative integers\n",
    "          Input array. (e.g. Raw avalanche size data in Oslo model.)\n",
    "\n",
    "    scale: float, greater or equal to 1.\n",
    "          Scale parameter controlling the growth of bin sizes.\n",
    "          If scale = 1., function will return frequency of each unique integer\n",
    "          value in data with no binning.\n",
    "\n",
    "    zeros: boolean\n",
    "          Set zeros = True if you want binning function to consider events of\n",
    "          size 0.\n",
    "          Note that output cannot be plotted on log-log scale if data contains\n",
    "          zeros. If zeros = False, events of size 0 will be removed from data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    x: array_like, 1 dimensional\n",
    "          Array of coordinates for bin centres calculated using geometric mean\n",
    "          of bin edges. Bins with a count of 0 will not be returned.\n",
    "    y: array_like, 1 dimensional\n",
    "          Array of normalised frequency counts within each bin. Bins with a\n",
    "          count of 0 will not be returned.\n",
    "    \"\"\"\n",
    "    if scale < 1:\n",
    "        raise ValueError('Function requires scale >= 1.')\n",
    "    count = np.bincount(data)\n",
    "    tot = np.sum(count)\n",
    "    smax = np.max(data)\n",
    "    if scale > 1:\n",
    "        jmax = np.ceil(np.log(smax)/np.log(scale))\n",
    "        if zeros:\n",
    "            binedges = scale ** np.arange(jmax + 1)\n",
    "            binedges[0] = 0\n",
    "        else:\n",
    "            binedges = scale ** np.arange(1,jmax + 1)\n",
    "            # count = count[1:]\n",
    "        binedges = np.unique(binedges.astype('uint64'))\n",
    "        x = (binedges[:-1] * (binedges[1:]-1)) ** 0.5\n",
    "        y = np.zeros_like(x)\n",
    "        count = count.astype('float')\n",
    "        for i in range(len(y)):\n",
    "            y[i] = np.sum(count[binedges[i]:binedges[i+1]]/(binedges[i+1] - binedges[i]))\n",
    "            # print(binedges[i],binedges[i+1])\n",
    "        # print(smax,jmax,binedges,x)\n",
    "        # print(x,y)\n",
    "    else:\n",
    "        x = np.nonzero(count)[0]\n",
    "        y = count[count != 0].astype('float')\n",
    "        if zeros != True and x[0] == 0:\n",
    "            x = x[1:]\n",
    "            y = y[1:]\n",
    "    y /= tot\n",
    "    x = x[y!=0]\n",
    "    y = y[y!=0]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fe8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, choice_parameters, h=0):\n",
    "        self.h = h\n",
    "        self.choice_parameters = choice_parameters\n",
    "        self.threshold_slope = choice(**choice_parameters)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.h + other.h\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self.h - other.h\n",
    "\n",
    "    def add_grain(self):\n",
    "        self.h = self.h + 1\n",
    "\n",
    "    def reset_th(self):\n",
    "        self.threshold_slope = choice(**self.choice_parameters)\n",
    "        \n",
    "    def topple_grain(self):\n",
    "        self.h = self.h - 1\n",
    "        self.reset_th()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.h = 0\n",
    "        self.reset_th()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baf9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pile:\n",
    "    def __init__(self, length, probs, threshold_zs):\n",
    "        choice_args = dict(a=threshold_zs, p=probs)\n",
    "        self.length = length\n",
    "        self.ava_size = 0\n",
    "        self.lattice = np.array([Site(choice_args) for _ in range(length)])\n",
    "        self.is_at_steady_state = False\n",
    "\n",
    "    def reset(self):\n",
    "        for site in self.lattice:\n",
    "            site.reset()\n",
    "        self.is_at_steady_state = False\n",
    "\n",
    "    def get_heights(self):\n",
    "        return [i.h for i in self.lattice]\n",
    "    \n",
    "    def get_pile_height(self):\n",
    "        return self.lattice[0].h\n",
    "\n",
    "    def get_threshold_slopes(self):\n",
    "        return [i.threshold_slope for i in self.lattice]\n",
    "\n",
    "    def find_unstable_site_indices(self):\n",
    "        current_slopes = np.append(self.lattice[:-1] - self.lattice[1:], self.lattice[-1].h)\n",
    "        return [i for i, site in enumerate(self.lattice) if current_slopes[i] > site.threshold_slope]\n",
    "    \n",
    "    def relax(self, site_index):\n",
    "        self.lattice[site_index].topple_grain()\n",
    "        self.ava_size = self.ava_size + 1\n",
    "        stop_len = site_index + 1\n",
    "\n",
    "        if stop_len == self.length:\n",
    "            self.is_at_steady_state = True\n",
    "            return\n",
    "        self.lattice[stop_len].add_grain()\n",
    "\n",
    "    def drop_grain(self, site_index=0):\n",
    "        self.ava_size = 0\n",
    "        self.lattice[site_index].add_grain()\n",
    "\n",
    "        while True:\n",
    "            unstable_site_indices = self.find_unstable_site_indices()\n",
    "            if not unstable_site_indices:\n",
    "                break\n",
    "            for i in unstable_site_indices:\n",
    "                self.relax(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12de3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the values project will use for Oslo model algorithm\n",
    "TH_SLOPES = (1, 2)\n",
    "PROBS = (0.5, 0.5)\n",
    "\n",
    "# define plot aesthetics\n",
    "FS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460182ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82037f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oslo25 = Pile(25, PROBS, TH_SLOPES)\n",
    "oslo25.get_threshold_slopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5068f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROBS_REGIME_2 = (0.0, 1.0)\n",
    "oslo_regime2 = Pile(25, PROBS_REGIME_2, TH_SLOPES)\n",
    "1 in oslo_regime2.get_threshold_slopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e4a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(oslo25.get_threshold_slopes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab27fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "oslo16 = Pile(16, PROBS, TH_SLOPES)\n",
    "oslo32 = Pile(32, PROBS, TH_SLOPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef82b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.135"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights_L16 = []\n",
    "\n",
    "for i in range(5000):\n",
    "    oslo16.drop_grain()\n",
    "    heights_L16.append(oslo16.get_pile_height())\n",
    " \n",
    "np.average(heights_L16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "963c487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.75326666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights_L32 = []\n",
    "\n",
    "for i in range(15000):\n",
    "    oslo32.drop_grain()\n",
    "    heights_L32.append(oslo32.get_pile_height())\n",
    "    \n",
    "np.average(heights_L32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53282c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038442301727555576"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem(heights_L16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46380d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04650353767276801"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem(heights_L32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f0b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 The height of the pile $h(t;L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e2f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "oslo4 = Pile(4, PROBS, TH_SLOPES)\n",
    "oslo8 = Pile(8, PROBS, TH_SLOPES)\n",
    "oslo16 = Pile(16, PROBS, TH_SLOPES)\n",
    "oslo32 = Pile(32, PROBS, TH_SLOPES)\n",
    "oslo64 = Pile(64, PROBS, TH_SLOPES)\n",
    "oslo128 = Pile(128, PROBS, TH_SLOPES)\n",
    "oslo256 = Pile(256, PROBS, TH_SLOPES)\n",
    "oslo512 = Pile(512, PROBS, TH_SLOPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447af2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "piles_set = (oslo4, oslo8, oslo16, oslo32, oslo64, oslo128, oslo256, oslo512)\n",
    "data_dict = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac7e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2bf1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 200000/200000 [00:09<00:00, 20755.31it/s]\n",
      "100%|████████████████████████████████| 200000/200000 [00:17<00:00, 11232.80it/s]\n",
      "100%|█████████████████████████████████| 200000/200000 [00:34<00:00, 5747.81it/s]\n",
      "100%|█████████████████████████████████| 200000/200000 [01:08<00:00, 2939.69it/s]\n",
      "100%|█████████████████████████████████| 200000/200000 [02:14<00:00, 1485.18it/s]\n",
      "100%|██████████████████████████████████| 200000/200000 [04:31<00:00, 736.39it/s]\n",
      "100%|██████████████████████████████████| 200000/200000 [09:18<00:00, 358.03it/s]\n",
      "100%|██████████████████████████████████| 200000/200000 [19:51<00:00, 167.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for pile in piles_set:\n",
    "    pile_info = {'h': [], 'asz': []}\n",
    "    while not pile.is_at_steady_state:\n",
    "        pile_info['h'].append(pile.get_pile_height())\n",
    "        pile_info['asz'].append(pile.ava_size)\n",
    "        pile.drop_grain()\n",
    "    for i in tqdm(range(iterations)):\n",
    "        pile_info['h'].append(pile.get_pile_height())\n",
    "        pile_info['asz'].append(pile.ava_size)\n",
    "        pile.drop_grain()\n",
    "    data_dict[pile.length] = pile_info\n",
    "pickle.dump(data_dict, open('data_file', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a987fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m steady_state_time_period \u001b[38;5;241m=\u001b[39m iterations\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_file'"
     ]
    }
   ],
   "source": [
    "data_dict = pickle.load(open('data_file', 'rb'))\n",
    "steady_state_time_period = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for length, pile_info in data_dict.items():\n",
    "    plt.plot(pile_info['h'], ':', label=length)\n",
    "\n",
    "plt.title(\"Pile height $ h(t; L) $\")\n",
    "plt.xlabel(\"$ t $\", fontsize=FS)\n",
    "plt.xlim(0, 275000)\n",
    "plt.ylabel(\"$ h $\", fontsize=FS)\n",
    "plt.legend(loc=4, title=\"System size $ L $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_moving_average(data, temporal_window=50):\n",
    "    window = np.ones(temporal_window) / temporal_window\n",
    "    return np.convolve(data, window, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ab2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = 50\n",
    "t_0 = int(t_range / 2)\n",
    "\n",
    "for length, pile_info in data_dict.items():\n",
    "    smooth_data = np_moving_average(pile_info['h'], t_range)\n",
    "    times = np.arange(t_0, len(smooth_data) + t_0)\n",
    "    \n",
    "    # plot results over a log-log scale\n",
    "    plt.loglog(times, smooth_data, ':', label = length)\n",
    "\n",
    "# plot set-up\n",
    "plt.title(\"Pile height $h(t; L) $\")\n",
    "plt.xlabel(\"$t$\", fontsize=FS)\n",
    "plt.ylabel(\"$h$\", fontsize=FS)\n",
    "plt.xlim(10e0, 10e6)\n",
    "plt.legend(loc=4, title=\"System Size $ L $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for length, pile_info in data_dict.items():\n",
    "    # use moving average method from previouly\n",
    "    smooth_h = np_moving_average(pile_info['h'], t_range)\n",
    "    \n",
    "    # collapse averages\n",
    "    smooth_h_collapse = smooth_h / length\n",
    "    t_collapse = np.arange(t_0, len(smooth_h_collapse) + t_0) / length**2\n",
    "    \n",
    "    # plot results of data collapse\n",
    "    plt.loglog(t_collapse, smooth_h_collapse, label=length)\n",
    "    \n",
    "# proportionality relationship for axes   \n",
    "plt.title(\"Data collapse of smoothed pile heights\") \n",
    "plt.xlabel(\"$ t/L^2 $\", fontsize=FS)\n",
    "plt.ylabel(\"$ \\widetilde h/L $\", fontsize=FS)\n",
    "plt.legend(title=\"System Size $ L $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebaa516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a function variable t, a constant coefficient a, and an exponent (power) k\n",
    "power_law = lambda t, a, k: a * t ** k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0 = 1000\n",
    "h_values = data_dict[64]['h'][t_0:-steady_state_time_period]\n",
    "\n",
    "offset_len = t_0 + len(h_values)\n",
    "t = np.arange(t_0, offset_len)\n",
    "\n",
    "(a, k), covm = curve_fit(power_law, t, h_values)\n",
    "a, k, np.sqrt(np.diag(covm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in the same graph plot the data for h_tilde\n",
    "plt.plot(t, h_values, label='Smoothed data', color='orange')\n",
    "\n",
    "# plot the power law fit\n",
    "plt.plot(t, power_law(t, a, k), label='Power law fit', color='red')\n",
    "\n",
    "plt.title(\"Power law fit of data for smoothed pile heights\")\n",
    "plt.xlabel(\"Times $t$\", fontsize=FS)\n",
    "plt.ylabel(\"Heights $h$\", fontsize=FS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relative difference of the fit and data\n",
    "diff = power_law(t, a, k) - h_values\n",
    "rel_diff = diff / h_values\n",
    "# plt.plot(t, abs(rel_diff), color='green')\n",
    "plt.plot(t, rel_diff, color='green')\n",
    "\n",
    "plt.title(\"Relative difference between height data and fit\")\n",
    "plt.xlabel(\"$t$\", fontsize=FS)\n",
    "plt.ylabel(\"Percent difference\", fontsize=FS)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array(list(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e63dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average heights\n",
    "avg_h = [np.average(pile_info['h'][-steady_state_time_period:]) for pile_info in data_dict.values()]\n",
    "\n",
    "plt.plot(lengths, avg_h, '.', color='blue')\n",
    "\n",
    "plt.xlabel(\"$ L $\", fontsize=FS)\n",
    "plt.ylabel(r\"$ \\langle h \\rangle $\", fontsize=FS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a881b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviations\n",
    "std_devs = [np.std(pile_info['h'][-steady_state_time_period:]) for pile_info in data_dict.values()]\n",
    "\n",
    "# plot results over a log-log scale\n",
    "plt.loglog(lengths, std_devs, '.', color='blue', label='data')\n",
    "\n",
    "plt.xlabel(\"$ L $\", fontsize=FS)\n",
    "plt.ylabel(\"$ \\sigma_h(L) $\", fontsize=FS)\n",
    "plt.xlim(10, 10e2)\n",
    "plt.ylim(0, 10e0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49943585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_height_probability(height_data):\n",
    "    total_time = len(height_data)\n",
    "    height_frequencies = sorted(Counter(height_data).items())\n",
    "    height_probabilities = OrderedDict()\n",
    "    for (key, value) in height_frequencies:\n",
    "        height_probabilities[key] = value / total_time\n",
    "    return height_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for length, pile_info in data_dict.items():\n",
    "    h_values = pile_info['h'][-steady_state_time_period:]\n",
    "    h_prob = calculate_height_probability(h_values)\n",
    "    heights = list(h_prob.keys())\n",
    "    probs = list(h_prob.values())\n",
    "    plt.plot(heights, probs, '-', label=length, linewidth=0.5)\n",
    "    \n",
    "plt.title(\"Height probability $P(h;L)$ for systems $L_i$\")\n",
    "plt.legend(title=\"System Size (L)\",  framealpha=0.8, prop={'size':10})\n",
    "plt.xlabel(\"$ h $\", fontsize=FS)\n",
    "plt.ylabel(\"$ P(h;L) $\", fontsize=FS)\n",
    "plt.xlim(-100,1000)\n",
    "plt.ylim(0,.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_avg_h = lambda L, a_0, a_1, om_1: a_0 * L * (1 - a_1 * L ** (-om_1))\n",
    "(a_0, a_1, om_1), covm = curve_fit(calc_avg_h, lengths, avg_h, absolute_sigma=True)\n",
    "a_0, a_1, om_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.58\n",
    "om = 0.24\n",
    "range_l = np.arange(1, 512)\n",
    "plt.loglog(lengths, std_devs, '.', color='blue', label='Data for system size $L=512$')\n",
    "plt.loglog(range_l, power_law(range_l, a, om), color='black', label='Power law fit with $a = 0.58, \\omega=0.25$')\n",
    "\n",
    "plt.title(\"Scaled $\\sigma_h(L) $ and its power law approximation\")\n",
    "plt.xlabel(\"$ L $\", fontsize=FS)\n",
    "plt.ylabel(\"$ \\sigma_h(L) $\", fontsize=FS)\n",
    "plt.xlim(10, 10e2)\n",
    "plt.ylim(10e-2, 10e0)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, w), covm = curve_fit(power_law, lengths, std_devs)\n",
    "\n",
    "a, w, np.sqrt(np.diag(covm))\n",
    "\n",
    "for length, pile_info in data_dict.items():\n",
    "    height_prob_dict = calculate_height_probability(pile_info['h'][-steady_state_time_period:])\n",
    "    collapsed_h = (np.array(list(height_prob_dict.keys()))-avg_h[list(data_dict.keys()).index(length)]) / length ** w \n",
    "    collapsed_p = np.array(list(height_prob_dict.values())) * length ** w\n",
    "    plt.plot(collapsed_h, collapsed_p, label=length, linewidth=0.5)\n",
    "\n",
    "plt.title(\"Data collapse of measured height probability $P(h;L)$\")\n",
    "plt.legend(title=\"System Size (L)\")\n",
    "plt.xlabel(r\"$(h - \\langle h \\rangle) L^{-0.24}$\", fontsize=FS)\n",
    "plt.ylabel(\"$L^{0.24}P(h; L)$\", fontsize=FS)\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(0,.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ae692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_log_bins(data):\n",
    "    centers, probabilities = logbin(data, scale=1.2, zeros=True)\n",
    "    return np.array(centers), np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ava_size_256 = data_dict[256]['asz']\n",
    "\n",
    "no_of_samples = 1000000\n",
    "ava_size_prob_dict = calculate_height_probability(ava_size_256[-i:])\n",
    "plt.loglog(list(ava_size_prob_dict.keys()), list(ava_size_prob_dict.values()), '.', ms=1.5, label=\"$N=1000000$\", color=\"blue\")\n",
    "centers, probs = make_log_bins(ava_size_256[-i:])\n",
    "plt.loglog(centers, probs, '-', color=\"pink\")\n",
    "plt.xlabel(\"$s$\", fontsize=FS)\n",
    "plt.ylabel(\"$P_N(s;L)$\", fontsize=FS)\n",
    "plt.legend(loc=3)\n",
    "plt.xlim(10e-1,10e5)\n",
    "plt.ylim(10e-10,10e-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_samples = 1000000\n",
    "\n",
    "for length, pile_info in data_dict.items():\n",
    "    centers, probabilities = make_log_bins(pile_info['asz'][-no_of_samples:])\n",
    "    plt.loglog(centers, probabilities, '-', label=length, linewidth=0.5)\n",
    "\n",
    "plt.legend(loc=3, title='System Size (L)', prop={'size':10})\n",
    "plt.xlabel(\"$s$\", fontsize=14)\n",
    "plt.ylabel(r\"$\\widetilde P_N(s;L)$\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c419cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, probabilities = make_log_bins(data_dict[512]['asz'][-no_of_samples:])\n",
    "power_law = lambda s, a, tau_s: a * s ** tau_s\n",
    "\n",
    "(a, tau_s), cov = curve_fit(power_law, centers[22:-10], probabilities[22:-10])\n",
    "\n",
    "a, tau_s, np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5266af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for length, pile_info in data_dict.items():\n",
    "    centers, probabilities = make_log_bins(pile_info['asz'][-no_of_samples:])\n",
    "    s_tau_probabilities = centers ** -tau_s * probabilities\n",
    "    plt.loglog(centers, s_tau_probabilities, '-', label=length, linewidth=0.5)\n",
    "\n",
    "plt.legend(loc=0, title='System Size (L)', prop={'size':10})\n",
    "plt.xlabel(\"$s$\", fontsize=14)\n",
    "plt.ylabel(r\"$s^{\\tau_s} \\widetilde P_N(s;L)$\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ca663",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2.25\n",
    "\n",
    "for length, pile_info in data_dict.items():\n",
    "    centers, probabilities = make_log_bins(pile_info['asz'][-no_of_samples:])\n",
    "    center_L_minus_Ds = centers / (length ** D)\n",
    "    s_tau_probabilities = centers ** -tau_s * probabilities\n",
    "    plt.loglog(center_L_minus_Ds, s_tau_probabilities, '-', label=length, linewidth=0.5)\n",
    "        \n",
    "plt.legend(title='System size $L$')\n",
    "plt.xlabel(r\"$s/L^{(D = 2.25)}$\", fontsize=FS)\n",
    "plt.ylabel(r\"$s^{(\\tau_s = 1.55)} \\cdot \\widetilde P_N(s;L)$\", fontsize=14)\n",
    "plt.xlim(10e-4,10e0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3b: Measuring directly the $ k $th moment $ \\langle s_k \\rangle $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_set = (1, 2, 3, 4)\n",
    "kth_moment_list = []\n",
    "\n",
    "for k in k_set:\n",
    "    kth_moments = []\n",
    "    for length, pile_info in data_dict.items():\n",
    "        kth_moment = np.average(np.array(pile_info['asz'][-steady_state_time_period:], dtype='float64') ** k)\n",
    "        kth_moments.append([length, kth_moment])\n",
    "    kth_moment_list.append(kth_moments)\n",
    "    \n",
    "kth_moment_array = np.array(kth_moment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a44e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(k_set):\n",
    "    plt.loglog(kth_moment_array[i, :, 0], kth_moment_array[i, :, 1], '.', label='k = {}'.format(k))\n",
    "\n",
    "plt.xlabel('$ L $', fontsize=FS)\n",
    "plt.ylabel(r'$\\langle s^k \\rangle$', fontsize=FS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = lambda x, c, m: m * x + c\n",
    "\n",
    "Ls = np.arange(0, 1000)\n",
    "k_slopes = []\n",
    "k_slope_errs = []\n",
    "\n",
    "for i, k in enumerate(k_set):\n",
    "    plt.loglog(kth_moment_array[i, :, 0], kth_moment_array[i, :, 1], '.', label='k = {}'.format(k))\n",
    "    (log_a, exp), cov = curve_fit(linear_regression, np.log(kth_moment_array[i, -3:, 0]), np.log(kth_moment_array[i, -3:, 1]))\n",
    "    plt.loglog(Ls, power_law(Ls, np.exp(log_a), exp), '--', color='black', linewidth=0.8)\n",
    "    k_slopes.append(exp)\n",
    "    k_slope_errs.append(np.sqrt(np.diag(cov)[1]))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('$L$', fontsize=14)\n",
    "plt.ylabel(r'$\\langle s^k \\rangle$', fontsize=FS)\n",
    "plt.ylim(10e-5,10e22)\n",
    "plt.xlim(10e-1,10e2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc63032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = (1, 2, 3, 4)\n",
    "(c, D), cov = curve_fit(linear_regression, ks, k_slopes, sigma=k_slope_errs)\n",
    "print(c, D, np.diag((cov)[1]))\n",
    "\n",
    "tau_s = 1 - c / D\n",
    "print(tau_s)\n",
    "\n",
    "plt.errorbar(ks, k_slopes, yerr=k_slope_errs, color='red', fmt='.', label='data', ms=2.0)\n",
    "\n",
    "ks_ = np.arange(6)\n",
    "plt.plot(ks_, linear_regression(ks_, c, D), color='orange', label='fit', linewidth=0.5)\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('$k$', fontsize=14)\n",
    "plt.ylabel(r'$\\phi$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f053037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
